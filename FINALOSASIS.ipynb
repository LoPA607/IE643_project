{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision diffusers transformers lpips accelerate\n",
        "!pip install git+https://github.com/huggingface/diffusers.git\n"
      ],
      "metadata": {
        "id": "IcxtSNgNHJwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Use GPU if available, else CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "01eYZxukHrPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoencoderKL\n",
        "\n",
        "# Pretrained latent autoencoder (used in Stable Diffusion)\n",
        "vq_model = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
        "vq_model.eval()\n",
        "\n",
        "def encode_content(img_tensor):\n",
        "    \"\"\"\n",
        "    Encode clean image into semantic latent using pretrained VQ-Diffusion autoencoder.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        latent = vq_model.encode(img_tensor).latent_dist.sample()\n",
        "    return latent\n"
      ],
      "metadata": {
        "id": "mG0ZV7yhHLR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "class SPN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-2])  # keep spatial features\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "spn = SPN().to(device)\n"
      ],
      "metadata": {
        "id": "REVmO7fiHNgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single reference image latent (random initialization)\n",
        "z_style = torch.randn((1, 4, 64, 64), device=device, requires_grad=True)\n",
        "optimizer = torch.optim.LBFGS([z_style], lr=0.01)\n"
      ],
      "metadata": {
        "id": "vX6PdYBMHPHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "# pipe.safety_checker = lambda images, **kwargs: (images, False)\n",
        "# Modify the safety checker to return a list of booleans\n",
        "pipe.safety_checker = lambda images, **kwargs: (images, [False])"
      ],
      "metadata": {
        "id": "eSoJKgRmHyyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def clip_directional_loss(img_out, img_style, img_clean):\n",
        "    # Clamp strictly to [0,1]\n",
        "    img_out = torch.clamp(img_out, 0.0, 1.0)\n",
        "    img_style = torch.clamp(img_style, 0.0, 1.0)\n",
        "    img_clean = torch.clamp(img_clean, 0.0, 1.0)\n",
        "\n",
        "    # Convert tensors to PIL Images (RGB)\n",
        "    tensor_to_pil = transforms.ToPILImage()\n",
        "    img_out_pil = tensor_to_pil(img_out.squeeze(0).cpu()).convert(\"RGB\")\n",
        "    img_style_pil = tensor_to_pil(img_style.squeeze(0).cpu()).convert(\"RGB\")\n",
        "    img_clean_pil = tensor_to_pil(img_clean.squeeze(0).cpu()).convert(\"RGB\")\n",
        "\n",
        "    # Now use clip_processor with PIL images\n",
        "    inputs_out = clip_processor(images=img_out_pil, return_tensors=\"pt\").to(device)\n",
        "    inputs_style = clip_processor(images=img_style_pil, return_tensors=\"pt\").to(device)\n",
        "    inputs_clean = clip_processor(images=img_clean_pil, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    emb_out = clip_model.get_image_features(**inputs_out)\n",
        "    emb_style = clip_model.get_image_features(**inputs_style)\n",
        "    emb_clean = clip_model.get_image_features(**inputs_clean)\n",
        "\n",
        "    dir_out = emb_out - emb_clean\n",
        "    dir_style = emb_style - emb_clean\n",
        "\n",
        "    dir_out = dir_out / dir_out.norm(dim=-1, keepdim=True)\n",
        "    dir_style = dir_style / dir_style.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    loss = 1 - (dir_out * dir_style).sum()\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "pBHFu5v5H2Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load image as PIL.Image (RGB) for Stable Diffusion\n",
        "def load_pil_image(path, size=512):\n",
        "    img = Image.open(path).convert(\"RGB\")   # ensure RGB mode\n",
        "    img = img.resize((size, size))\n",
        "    return img\n",
        "\n",
        "# Convert tensor [1,3,H,W] to PIL.Image\n",
        "def tensor_to_pil(tensor):\n",
        "    tensor = tensor.detach().cpu().squeeze(0).clamp(0,1)\n",
        "    pil_img = transforms.ToPILImage()(tensor)\n",
        "    return pil_img.convert(\"RGB\")\n",
        "\n",
        "# Load image as tensor for SPN / CLIP (normalized [0,1])\n",
        "def load_tensor_image(path, size=512):\n",
        "    pil_img = load_pil_image(path, size)\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(pil_img).unsqueeze(0)  # [1,3,H,W]\n",
        "    return tensor.to(device)\n"
      ],
      "metadata": {
        "id": "ZSZVNfacIipW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PIL images for Stable Diffusion\n",
        "pil_clean = load_pil_image(\"/content/clean.jpg\")\n",
        "pil_style = load_pil_image(\"/content/hidemotionblur.jpg\")\n",
        "\n",
        "# Tensors for SPN / CLIP\n",
        "clean_tensor = load_tensor_image(\"/content/clean.jpg\")  # GPU\n",
        "style_tensor = load_tensor_image(\"/content/hidemotionblur.jpg\")  # GPU\n",
        "\n",
        "# Ensure z_style is on same device\n",
        "z_style = z_style.to(device)\n"
      ],
      "metadata": {
        "id": "rrCTuG43J8id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models\n",
        "vq_model = vq_model.to(device)\n",
        "spn = spn.to(device)\n",
        "clip_model = clip_model.to(device)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# Tensors\n"
      ],
      "metadata": {
        "id": "sHHJpma0IoLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# Compute SPN / VQ latent features (GPU tensors)\n",
        "z_struct = spn(clean_tensor)\n",
        "z_content = encode_content(clean_tensor)\n",
        "\n",
        "optimizer = torch.optim.Adam([z_style], lr=1e-4)\n",
        "\n",
        "for step in range(50):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Stable Diffusion Img2Img expects PIL.Image (RGB, CPU)\n",
        "    # Convert the clean_tensor to a PIL Image\n",
        "    clean_pil_image = tensor_to_pil(clean_tensor)\n",
        "\n",
        "    output_imgs = pipe(\n",
        "        prompt=\"\",\n",
        "        image=clean_pil_image,     # PIL.Image, CPU\n",
        "        strength=0.4,\n",
        "        guidance_scale=12,\n",
        "        num_inference_steps=50\n",
        "    ).images\n",
        "\n",
        "    # Convert output back to GPU tensor for loss calculation\n",
        "    output_img = transforms.ToTensor()(output_imgs[0]).unsqueeze(0).to(device)\n",
        "\n",
        "    # Clamp pixel values to [0, 1] before passing to CLIP directional loss\n",
        "    output_img_clamped = torch.clamp(output_img, 0.0, 1.0).float()\n",
        "    style_tensor_clamped = torch.clamp(style_tensor, 0.0, 1.0).float()\n",
        "    clean_tensor_clamped = torch.clamp(clean_tensor, 0.0, 1.0).float()\n",
        "\n",
        "\n",
        "\n",
        "    # Compute losses\n",
        "    content_loss = mse_loss(output_img, clean_tensor)\n",
        "    style_loss = mse_loss(output_img, style_tensor)\n",
        "    clip_loss_val = clip_directional_loss(output_img_clamped, style_tensor_clamped, clean_tensor_clamped)\n",
        "\n",
        "\n",
        "    total_loss = 1*content_loss + 100*style_loss + 100*clip_loss_val\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Step {step}, Total Loss: {total_loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "nWzsqs7pH4z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------\n",
        "# One-shot finalization and metrics\n",
        "# ------------------------\n",
        "import os\n",
        "import torch\n",
        "import lpips\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import torchvision.transforms as T\n",
        "from IPython.display import display\n",
        "\n",
        "out_dir = \"/content/osasis_outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# Final stylized output (from last iteration of your loop)\n",
        "final_pil = output_imgs[0]\n",
        "final_path = os.path.join(out_dir, \"stylized_final.png\")\n",
        "final_pil.save(final_path)\n",
        "print(\"âœ… Saved stylized image:\", final_path)\n",
        "\n",
        "# ---- Metrics ----\n",
        "to_tensor = T.ToTensor()\n",
        "to_norm_lpips = lambda t: (t * 2.0 - 1.0)  # map [0,1] -> [-1,1]\n",
        "\n",
        "lpips_fn = lpips.LPIPS(net='vgg').to(device)\n",
        "\n",
        "stylized_tensor = to_tensor(final_pil).unsqueeze(0).to(device)\n",
        "clean_tensor_for_eval = clean_tensor.clone()\n",
        "\n",
        "lpips_score = lpips_fn(\n",
        "    to_norm_lpips(stylized_tensor),\n",
        "    to_norm_lpips(clean_tensor_for_eval)\n",
        ").item()\n",
        "\n",
        "styl_np = np.array(final_pil).astype(np.float32) / 255.0\n",
        "clean_np = np.transpose(clean_tensor_for_eval.squeeze(0).cpu().numpy(), (1,2,0))\n",
        "ssim_score = ssim(styl_np, clean_np, channel_axis=2, data_range=1.0) # Use channel_axis instead of multichannel\n",
        "\n",
        "print(f\"ðŸ“Š LPIPS (stylized vs clean): {lpips_score:.4f}\")\n",
        "print(f\"ðŸ“Š SSIM  (stylized vs clean): {ssim_score:.4f}\")\n",
        "\n",
        "# ---- Visual sanity check ----\n",
        "print(\"Displaying final generated image:\")\n",
        "display(final_pil)"
      ],
      "metadata": {
        "id": "s6I3upd7KS8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a6af637"
      },
      "source": [
        "### Gram Matrix Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "456c3dcf"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def gram_matrix(input):\n",
        "    a, b, c, d = input.size()  # a=batch size(=1), b=number of feature maps, (c,d)=dimensions of a f. map (N=c*d)\n",
        "\n",
        "    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
        "\n",
        "    G = torch.mm(features, features.t())  # compute the gram product\n",
        "\n",
        "    return G.div(a * b * c * d) # normalize the values of the gram matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cc18e2e"
      },
      "source": [
        "# Calculate Gram matrices for the stylized and style images using the SPN model's features\n",
        "stylized_features = spn(stylized_tensor)\n",
        "style_features = spn(style_tensor)\n",
        "\n",
        "gram_stylized = gram_matrix(stylized_features)\n",
        "gram_style = gram_matrix(style_features)\n",
        "\n",
        "print(\"âœ… Calculated Gram matrices for stylized and style images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c674e84a"
      },
      "source": [
        "### Gram Matrix Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da6b5dad"
      },
      "source": [
        "# Calculate the distance between the Gram matrices (e.g., using Mean Squared Error)\n",
        "gram_loss = mse_loss(gram_stylized, gram_style)\n",
        "\n",
        "print(f\"ðŸ“Š Gram Matrix Loss (stylized vs style): {gram_loss.item():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4eLK95Kg7sQ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}