{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoPA607/IE643_project/blob/main/vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JIcQvyp75hw"
      },
      "outputs": [],
      "source": [
        "!pip install -q timm==0.9.2 torch torchvision pillow tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "import math"
      ],
      "metadata": {
        "id": "QQBuVlFe7924"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "IMG_SIZE = 224                 # ViT base patch16 standard size\n",
        "VIT_NAME = 'vit_base_patch16_224'\n",
        "EXTRACT_LAYERS = [2, 5, 8, 11] # layer indices to extract features from\n",
        "NUM_STEPS = 50\n",
        "LR = 0.002\n",
        "STYLE_WEIGHT = 1e9\n",
        "CONTENT_WEIGHT = 0.000001\n",
        "TV_WEIGHT = 1\n",
        "JITTER_MAX = 3              # max pixel jitter to apply each step (helps remove seams)\n",
        "MULTI_SCALE = [1.0, 0.75, 0.5] # optimize at these image scales (coarse->fine)\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_8PKPIf8AP0",
        "outputId": "e8308033-6ee0-4647-f7a4-94776ab0f4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x795c90c322b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(path, size=IMG_SIZE):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((size, size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    return transform(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "def denorm_tensor(tensor):\n",
        "    # input in [0,1], returns clamped [0,1]\n",
        "    return tensor.clamp(0., 1.)\n",
        "\n",
        "def save_out(tensor, path):\n",
        "    t = denorm_tensor(t.detach().cpu())\n",
        "    save_image(t, path)"
      ],
      "metadata": {
        "id": "TIB2zt9B8J2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTTokenExtractor:\n",
        "    def __init__(self, model, layers):\n",
        "        \"\"\"\n",
        "        model: timm ViT model\n",
        "        layers: list of transformer block indices to hook outputs from\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.layers = layers\n",
        "        self._hooks = []\n",
        "        self.outputs = {}\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _hook_fn(self, idx):\n",
        "        def hook(module, input, output):\n",
        "            # output shape (B, N+1, D) for timm ViT blocks; keep all tokens\n",
        "            self.outputs[idx] = output\n",
        "        return hook\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        # timm ViT typically has model.blocks\n",
        "        for idx in self.layers:\n",
        "            block = self.model.blocks[idx]\n",
        "            h = block.register_forward_hook(self._hook_fn(idx))\n",
        "            self._hooks.append(h)\n",
        "\n",
        "    def remove(self):\n",
        "        for h in self._hooks:\n",
        "            h.remove()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def features(self, x):\n",
        "        \"\"\"Run forward_features and return dict {layer_idx: tokens (B, N, D) excluding cls}\"\"\"\n",
        "        self.outputs.clear()\n",
        "        _ = self.model.forward_features(x)\n",
        "        feats = {}\n",
        "        for k, v in self.outputs.items():\n",
        "            # drop cls token at index 0 -> get patch tokens: (B, N, D)\n",
        "            tokens = v[:, 1:, :].detach()  # (B, N, D)\n",
        "            feats[k] = tokens\n",
        "        return feats"
      ],
      "metadata": {
        "id": "vA80Dcbx8VT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_tokens(tokens):\n",
        "    \"\"\"\n",
        "    tokens: (B, N, D)\n",
        "    returns gram: (B, D, D)\n",
        "    \"\"\"\n",
        "    B, N, D = tokens.shape\n",
        "    t = tokens.transpose(1, 2)  # (B, D, N)\n",
        "    g = torch.bmm(t, tokens) / (D * N)\n",
        "    return g"
      ],
      "metadata": {
        "id": "YUyaap8N8Z91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def total_variation_loss(x):\n",
        "    # x: (B, C, H, W), values in [0,1]\n",
        "    diff_x = x[:, :, :, 1:] - x[:, :, :, :-1]\n",
        "    diff_y = x[:, :, 1:, :] - x[:, :, :-1, :]\n",
        "    return torch.mean(torch.abs(diff_x)) + torch.mean(torch.abs(diff_y))"
      ],
      "metadata": {
        "id": "GowTfJes8cIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_jitter(img, max_jitter):\n",
        "    \"\"\"Random circular shift (wrap-around) by up to +/- max_jitter pixels in x and y.\"\"\"\n",
        "    if max_jitter <= 0:\n",
        "        return img, (0, 0)\n",
        "    B, C, H, W = img.shape\n",
        "    dx = random.randint(-max_jitter, max_jitter)\n",
        "    dy = random.randint(-max_jitter, max_jitter)\n",
        "    # roll performs wrap-around; helps avoid black borders\n",
        "    img_j = torch.roll(img, shifts=(dy, dx), dims=(2, 3))\n",
        "    return img_j, (dx, dy)\n",
        "\n",
        "def undo_jitter(img, shift):\n",
        "    dx, dy = shift\n",
        "    return torch.roll(img, shifts=(-dy, -dx), dims=(2, 3))"
      ],
      "metadata": {
        "id": "IJpzW6Io8gN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vit_style_transfer(content_path, style_path, out_prefix='vit_stylized',\n",
        "                       steps=NUM_STEPS, lr=LR,\n",
        "                       style_w=STYLE_WEIGHT, content_w=CONTENT_WEIGHT, tv_w=TV_WEIGHT):\n",
        "    # Load ViT\n",
        "    print(\"Loading ViT model:\", VIT_NAME)\n",
        "    vit = timm.create_model(VIT_NAME, pretrained=True).to(DEVICE).eval()\n",
        "    extractor = ViTTokenExtractor(vit, EXTRACT_LAYERS)\n",
        "\n",
        "    # Load images (we will rescale for multi-scale)\n",
        "    content_full = Image.open(content_path).convert('RGB')\n",
        "    style_full = Image.open(style_path).convert('RGB')\n",
        "\n",
        "    # Precompute style features at base scale (ViT expects specific size)\n",
        "    style_tensor_base = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])(style_full).unsqueeze(0).to(DEVICE)\n",
        "    style_tensor_base = normalize_imagenet(style_tensor_base)\n",
        "    style_feats_base = extractor.features(style_tensor_base)\n",
        "\n",
        "    # define initial image: start from content (resized at base scale)\n",
        "    base_size = IMG_SIZE\n",
        "    content_tensor = transforms.Compose([transforms.Resize((base_size, base_size)), transforms.ToTensor()])(content_full).unsqueeze(0).to(DEVICE)\n",
        "    content_tensor = normalize_imagenet(content_tensor)\n",
        "\n",
        "    # target: optimize pixels starting from content\n",
        "    target = content_tensor.clone().requires_grad_(True)\n",
        "\n",
        "    optimizer = optim.Adam([target], lr=lr)\n",
        "\n",
        "    # helper: feature extract for content at base scale\n",
        "    content_feats_base = extractor.features(content_tensor)\n",
        "\n",
        "    print(\"Starting optimization...\")\n",
        "    for step in tqdm(range(1, steps + 1)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Multi-scale losses: compute at each scale (coarse to fine)\n",
        "        total_style_loss = 0.0\n",
        "        total_content_loss = 0.0\n",
        "\n",
        "        for scale in MULTI_SCALE:\n",
        "            # resize target to current scale\n",
        "            s = max(32, int(IMG_SIZE * scale))\n",
        "            target_resized = nn.functional.interpolate(target, size=(s, s), mode='bilinear', align_corners=False)\n",
        "\n",
        "            # apply random jitter to break patch alignment\n",
        "            target_jittered, shift = random_jitter(target_resized, max_jitter=JITTER_MAX)\n",
        "\n",
        "            # extract features (tokens) for target_jittered\n",
        "            # NOTE: ViT feature extraction expects IMG_SIZE input, so resize here before extracting\n",
        "            target_for_feats = nn.functional.interpolate(target_jittered, size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n",
        "            feats_t = extractor.features(target_for_feats)  # dict layer->(B,N,D)\n",
        "\n",
        "\n",
        "            # content loss only at base scale (or we can compute across scales)\n",
        "            if math.isclose(scale, 1.0):\n",
        "                for k in content_feats_base:\n",
        "                    cf = content_feats_base[k]\n",
        "                    tf = feats_t[k]\n",
        "                    total_content_loss += nn.functional.mse_loss(tf, cf)\n",
        "\n",
        "            # style loss: compare Gram matrices at this scale between target and precomputed style\n",
        "            # NOTE: Style feats were precomputed at base scale, so use those\n",
        "            style_feats = style_feats_base # Use base scale style features\n",
        "            for k in style_feats:\n",
        "                gram_t = gram_tokens(feats_t[k])\n",
        "                gram_s = gram_tokens(style_feats[k])\n",
        "                total_style_loss += nn.functional.mse_loss(gram_t, gram_s)\n",
        "\n",
        "        # normalize by number of scales and layers to keep magnitudes stable\n",
        "        n_scales = len(MULTI_SCALE)\n",
        "        n_layers = len(EXTRACT_LAYERS)\n",
        "        total_style_loss = total_style_loss / (n_scales * n_layers)\n",
        "        total_content_loss = total_content_loss / max(1.0, n_layers)  # content computed only at base\n",
        "\n",
        "        # total variation smoothing\n",
        "        tv_loss = total_variation_loss(target)\n",
        "\n",
        "        loss = content_w * total_content_loss + style_w * total_style_loss + tv_w * tv_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ensure target remains in [0,1] before normalization\n",
        "        with torch.no_grad():\n",
        "            # undo normalization clamping: target is in normalized domain; map to 0..1 then renormalize\n",
        "            t = denormalize_imagenet(target)\n",
        "            t = t.clamp(0.0, 1.0)\n",
        "            target.data.copy_(normalize_imagenet(t))\n",
        "\n",
        "        if step % 50 == 0 or step == steps:\n",
        "            # save intermediate result (denormalize)\n",
        "            out_img = denormalize_imagenet(target).detach().cpu().squeeze(0)\n",
        "            save_image(out_img.clamp(0,1), f\"{out_prefix}_step{step}.png\")\n",
        "            print(f\"Step {step}: loss={loss.item():.4f} (style={total_style_loss.item():.4f}, content={total_content_loss.item():.4f}, tv={tv_loss.item():.6f})\")\n",
        "\n",
        "    extractor.remove()\n",
        "    final_img = denormalize_imagenet(target).detach().cpu().squeeze(0)\n",
        "    save_image(final_img.clamp(0,1), f\"{out_prefix}_final.png\")\n",
        "    print(\"Saved final:\", f\"{out_prefix}_final.png\")\n",
        "    return f\"{out_prefix}_final.png\""
      ],
      "metadata": {
        "id": "aGcwzCIr8hIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1,3,1,1)\n",
        "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1,3,1,1)\n",
        "\n",
        "def normalize_imagenet(x):\n",
        "    return (x - IMAGENET_MEAN) / IMAGENET_STD\n",
        "\n",
        "def denormalize_imagenet(x):\n",
        "    return x * IMAGENET_STD + IMAGENET_MEAN\n",
        "\n",
        "# -----------------------\n",
        "# Small helpers\n",
        "# -----------------------\n",
        "from torchvision.utils import save_image\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_image(path, title):\n",
        "    img = Image.open(path)\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(img)\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Run example (update paths)\n",
        "# -----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    content_path = \"content2.jpg\"   # change to your content path\n",
        "    style_path = \"style3.jpeg\"      # change to your style path\n",
        "    out = vit_style_transfer(content_path, style_path, out_prefix='vit_hybrid', steps=800)\n",
        "    print(\"Output saved to:\", out)\n",
        "\n",
        "    # Display images\n",
        "    display_image(content_path, \"Content Image\")\n",
        "    display_image(style_path, \"Style Image\")\n",
        "    display_image(out, \"Stylized Image\")"
      ],
      "metadata": {
        "id": "ESEMDg5I8lT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JkWxc56iBArM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}