{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoPA607/IE643_project/blob/main/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2tIgeZQOxDj"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade diffusers transformers accelerate safetensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me77rxt4Oyrw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Setup ===\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Using device: {device.upper()}\")\n",
        "\n",
        "def show_image(img, title=None):\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    if title: plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# === Load SD pipeline ===\n",
        "print(\"\\nüöÄ Loading Stable Diffusion pipeline...\")\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ").to(device)\n",
        "pipe.enable_attention_slicing()\n",
        "print(\"‚úÖ Pipeline loaded successfully!\\n\")\n",
        "\n",
        "\n",
        "# === Upload images ===\n",
        "print(\"üì§ Upload the STYLE image:\")\n",
        "uploaded_style = files.upload()\n",
        "style_path = list(uploaded_style.keys())[0]\n",
        "style_img = Image.open(style_path).convert(\"RGB\").resize((512,512))\n",
        "print(f\"‚úÖ Style image '{style_path}' loaded.\")\n",
        "\n",
        "print(\"\\nüì§ Upload the CONTENT image:\")\n",
        "uploaded_content = files.upload()\n",
        "content_path = list(uploaded_content.keys())[0]\n",
        "content_img = Image.open(content_path).convert(\"RGB\").resize((512,512))\n",
        "print(f\"‚úÖ Content image '{content_path}' loaded.\\n\")\n",
        "\n",
        "show_image(style_img, \"Style Image\")\n",
        "show_image(content_img, \"Content Image\")\n",
        "\n",
        "\n",
        "# === Encode both images into latents ===\n",
        "print(\"üîÑ Encoding both images into latent space...\")\n",
        "\n",
        "def encode_image(img):\n",
        "    img_tensor = pipe.feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(device, dtype=torch.float16)\n",
        "    latents = pipe.vae.encode(img_tensor).latent_dist.sample() * 0.18215\n",
        "    return latents\n",
        "\n",
        "style_latent = encode_image(style_img)\n",
        "content_latent = encode_image(content_img)\n",
        "print(\"‚úÖ Encoded both images into latents!\\n\")\n",
        "\n",
        "\n",
        "# === Blend latents ===\n",
        "alpha = 0.3  # 0.3 = light style, 0.7 = strong style\n",
        "print(f\"üßÆ Blending latents (style strength = {alpha}) ...\")\n",
        "blended_latent = (1 - alpha) * content_latent + alpha * style_latent\n",
        "print(\"‚úÖ Latents blended successfully!\\n\")\n",
        "\n",
        "\n",
        "# === Decode blended latent back to image ===\n",
        "print(\"üñºÔ∏è Decoding blended latent to get final image...\")\n",
        "with torch.no_grad():\n",
        "    decoded_img = pipe.vae.decode(blended_latent / 0.18215).sample\n",
        "\n",
        "decoded_img = (decoded_img / 2 + 0.5).clamp(0, 1)\n",
        "decoded_img = decoded_img.cpu().permute(0, 2, 3, 1).float().numpy()[0]\n",
        "decoded_img_pil = Image.fromarray((decoded_img * 255).astype(\"uint8\"))\n",
        "\n",
        "show_image(decoded_img_pil, \"Final Stylized Output\")\n",
        "decoded_img_pil.save(\"stylized_output.png\")\n",
        "\n",
        "# === Summary ===\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "axes[0].imshow(style_img)\n",
        "axes[0].set_title(\"Style Image\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "axes[1].imshow(content_img)\n",
        "axes[1].set_title(\"Content Image\")\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "axes[2].imshow(decoded_img_pil)\n",
        "axes[2].set_title(\"Final Stylized Output (Latent Blend)\")\n",
        "axes[2].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üíæ Saved as 'stylized_output.png'\")\n",
        "print(\"‚úÖ One-shot style transfer completed successfully! üé®‚ú®\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}