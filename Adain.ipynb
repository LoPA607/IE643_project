{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtOMQeUvXv-n",
        "outputId": "f5c707e3-a38c-425a-ae4c-f5770c5d5575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Iter 1/6000 | D_loss: 0.346559 | G_adv: 5.584313 | Content: 0.130698 | Style: 0.000005 | FM: 2.333939\n",
            "Iter 200/6000 | D_loss: 0.175266 | G_adv: 0.194975 | Content: 0.216334 | Style: 0.000001 | FM: 0.239352\n",
            "Iter 400/6000 | D_loss: 0.036701 | G_adv: 0.724440 | Content: 0.228337 | Style: 0.000000 | FM: 0.658393\n",
            "Iter 600/6000 | D_loss: 0.019636 | G_adv: 0.767407 | Content: 0.222822 | Style: 0.000000 | FM: 0.749331\n",
            "Iter 800/6000 | D_loss: 0.011885 | G_adv: 0.966678 | Content: 0.228439 | Style: 0.000000 | FM: 0.795267\n",
            "Iter 1000/6000 | D_loss: 0.195138 | G_adv: 0.217692 | Content: 0.209739 | Style: 0.000000 | FM: 0.061681\n",
            "Iter 1200/6000 | D_loss: 0.189582 | G_adv: 0.223911 | Content: 0.195090 | Style: 0.000000 | FM: 0.060032\n",
            "Iter 1400/6000 | D_loss: 0.198154 | G_adv: 0.230931 | Content: 0.194732 | Style: 0.000000 | FM: 0.059811\n",
            "Iter 1600/6000 | D_loss: 0.185414 | G_adv: 0.293772 | Content: 0.191995 | Style: 0.000000 | FM: 0.138206\n",
            "Iter 1800/6000 | D_loss: 0.218063 | G_adv: 0.353915 | Content: 0.192725 | Style: 0.000000 | FM: 0.185936\n",
            "Iter 2000/6000 | D_loss: 0.162131 | G_adv: 0.312455 | Content: 0.197591 | Style: 0.000000 | FM: 0.141632\n",
            "Iter 2200/6000 | D_loss: 0.135067 | G_adv: 0.363698 | Content: 0.205252 | Style: 0.000000 | FM: 0.238417\n",
            "Iter 2400/6000 | D_loss: 0.154068 | G_adv: 0.273041 | Content: 0.203789 | Style: 0.000000 | FM: 0.324225\n",
            "Iter 2600/6000 | D_loss: 0.119649 | G_adv: 0.296105 | Content: 0.215853 | Style: 0.000000 | FM: 0.381957\n",
            "Iter 2800/6000 | D_loss: 0.076327 | G_adv: 0.351938 | Content: 0.222205 | Style: 0.000000 | FM: 0.454375\n",
            "Iter 3000/6000 | D_loss: 0.015988 | G_adv: 0.575016 | Content: 0.221270 | Style: 0.000000 | FM: 0.650547\n",
            "Iter 3200/6000 | D_loss: 0.119455 | G_adv: 0.237655 | Content: 0.207078 | Style: 0.000000 | FM: 0.171387\n",
            "Iter 3400/6000 | D_loss: 0.036501 | G_adv: 0.260604 | Content: 0.220930 | Style: 0.000000 | FM: 0.522781\n",
            "Iter 3600/6000 | D_loss: 0.056717 | G_adv: 0.414698 | Content: 0.218107 | Style: 0.000000 | FM: 0.689185\n",
            "Iter 3800/6000 | D_loss: 0.028331 | G_adv: 0.707959 | Content: 0.218794 | Style: 0.000000 | FM: 0.773610\n",
            "Iter 4000/6000 | D_loss: 0.020850 | G_adv: 0.522423 | Content: 0.221235 | Style: 0.000000 | FM: 0.814181\n",
            "Iter 4200/6000 | D_loss: 0.035775 | G_adv: 0.393063 | Content: 0.221228 | Style: 0.000000 | FM: 0.606913\n",
            "Iter 4400/6000 | D_loss: 0.011122 | G_adv: 0.487407 | Content: 0.228217 | Style: 0.000000 | FM: 0.756823\n",
            "Iter 4600/6000 | D_loss: 0.011714 | G_adv: 0.647485 | Content: 0.224161 | Style: 0.000000 | FM: 0.830796\n",
            "Iter 4800/6000 | D_loss: 0.009528 | G_adv: 0.737024 | Content: 0.226677 | Style: 0.000000 | FM: 0.820828\n",
            "Iter 5000/6000 | D_loss: 0.293584 | G_adv: 0.397147 | Content: 0.213658 | Style: 0.000000 | FM: 0.094472\n",
            "Iter 5200/6000 | D_loss: 0.186409 | G_adv: 0.219643 | Content: 0.197737 | Style: 0.000000 | FM: 0.055214\n",
            "Iter 5400/6000 | D_loss: 0.161600 | G_adv: 0.230439 | Content: 0.199269 | Style: 0.000000 | FM: 0.179147\n",
            "Iter 5600/6000 | D_loss: 0.189696 | G_adv: 0.361531 | Content: 0.208386 | Style: 0.000000 | FM: 0.162344\n",
            "Iter 5800/6000 | D_loss: 0.037478 | G_adv: 0.281095 | Content: 0.223127 | Style: 0.000000 | FM: 0.458955\n",
            "Iter 6000/6000 | D_loss: 0.003995 | G_adv: 0.533143 | Content: 0.226543 | Style: 0.000000 | FM: 0.704341\n",
            "Done. Outputs saved to: ./oasis_one_shot_out01\n"
          ]
        }
      ],
      "source": [
        "# oasis_one_shot.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import random\n",
        "import os\n",
        "\n",
        "# -----------------------------\n",
        "# Config / Device\n",
        "# -----------------------------\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using device:', device)\n",
        "\n",
        "# -----------------------------\n",
        "# Paths (edit if needed)\n",
        "# -----------------------------\n",
        "style_path = '/content/dog.jpg'\n",
        "content_path = '/content/Screenshot from 2025-10-04 23-08-58.png'\n",
        "out_dir = './oasis_one_shot_out01'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Transforms / Load images\n",
        "# -----------------------------\n",
        "imsize = 256\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((imsize, imsize)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "content_img = Image.open(content_path).convert('RGB')\n",
        "style_img = Image.open(style_path).convert('RGB')\n",
        "\n",
        "content = transform(content_img).unsqueeze(0).to(device)  # [1,3,H,W]\n",
        "style = transform(style_img).unsqueeze(0).to(device)\n",
        "\n",
        "# optionally create simple augmentations to help discriminator generalize\n",
        "def augment(x):\n",
        "    # small random flips and color jitter\n",
        "    if random.random() < 0.5:\n",
        "        x = torch.flip(x, dims=[3])  # horizontal flip\n",
        "    return x\n",
        "\n",
        "# -----------------------------\n",
        "# VGG Encoder (same as yours)\n",
        "# -----------------------------\n",
        "vgg_pretrained = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "for p in vgg_pretrained.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "class VGGEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = {'3':'relu1_2','8':'relu2_2','17':'relu3_4','26':'relu4_4'}\n",
        "        self.vgg = vgg_pretrained\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = {}\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.layers:\n",
        "                features[self.layers[name]] = x\n",
        "        return features\n",
        "\n",
        "encoder = VGGEncoder().to(device)\n",
        "\n",
        "# -----------------------------\n",
        "# AdaIN function\n",
        "# -----------------------------\n",
        "def adain(content_feat, style_feat, eps=1e-5):\n",
        "    c_mean = content_feat.mean([2,3], keepdim=True)\n",
        "    c_std = content_feat.std([2,3], keepdim=True)\n",
        "    s_mean = style_feat.mean([2,3], keepdim=True)\n",
        "    s_std = style_feat.std([2,3], keepdim=True)\n",
        "    return s_std * (content_feat - c_mean) / (c_std + eps) + s_mean\n",
        "\n",
        "# -----------------------------\n",
        "# Decoder (same design as yours but slightly expanded)\n",
        "# -----------------------------\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=512):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(latent_dim, 256, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(64, 3, 3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "generator = Decoder(latent_dim=512).to(device)\n",
        "\n",
        "# -----------------------------\n",
        "# PatchGAN Discriminator (OASIS-ish fully conv)\n",
        "# -----------------------------\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, base=64):\n",
        "        super().__init__()\n",
        "        # simple PatchGAN: series of convs reducing resolution\n",
        "        def block(in_c, out_c, stride=2, norm=True):\n",
        "            layers = [nn.Conv2d(in_c, out_c, 4, stride=stride, padding=1)]\n",
        "            if norm:\n",
        "                layers.append(nn.BatchNorm2d(out_c))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            *block(in_channels, base, stride=2, norm=False),  # 128x128\n",
        "            *block(base, base*2, stride=2),  # 64x64\n",
        "            *block(base*2, base*4, stride=2), # 32x32\n",
        "            *block(base*4, base*8, stride=1), # 31x31 (patches)\n",
        "            nn.Conv2d(base*8, 1, 4, stride=1, padding=1)  # final single-channel patch map\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)  # e.g. [B,1,Hp,Wp]\n",
        "\n",
        "discriminator = PatchDiscriminator().to(device)\n",
        "\n",
        "# -----------------------------\n",
        "# Loss functions\n",
        "# -----------------------------\n",
        "mse = nn.MSELoss()\n",
        "bce = nn.BCEWithLogitsLoss()  # if we use logits\n",
        "l1 = nn.L1Loss()\n",
        "\n",
        "def gram_matrix(feat):\n",
        "    b, c, h, w = feat.size()\n",
        "    f = feat.view(b, c, h*w)\n",
        "    gram = torch.bmm(f, f.transpose(1,2)) / (c*h*w)\n",
        "    return gram\n",
        "\n",
        "# -----------------------------\n",
        "# Optimizers\n",
        "# -----------------------------\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=4e-4, betas=(0.5, 0.999))\n",
        "\n",
        "# -----------------------------\n",
        "# Training hyperparams\n",
        "# -----------------------------\n",
        "num_iters = 6000            # increase for better results\n",
        "print_every = 200\n",
        "adv_weight = 1.0            # adversarial weight\n",
        "content_weight = 1.0        # perceptual content loss weight\n",
        "style_weight = 1e6          # style Gram loss weight (you used large before)\n",
        "fm_weight = 10.0            # feature matching weight from discriminator (stabilizes)\n",
        "# label smoothing for discriminator\n",
        "real_label = 0.9\n",
        "fake_label = 0.0\n",
        "\n",
        "# Precompute encoded features for style and content (relu4_4 for AdaIN)\n",
        "with torch.no_grad():\n",
        "    content_feats_full = encoder(content)\n",
        "    style_feats_full = encoder(style)\n",
        "    content_feat = content_feats_full['relu4_4']\n",
        "    style_feat = style_feats_full['relu4_4']\n",
        "\n",
        "# For perceptual/content loss, we will compare relu4_4 features of output to original content_feat\n",
        "# The generator inputs: the AdaIN-mixed relu4_4 features -> decoder -> output\n",
        "\n",
        "# -----------------------------\n",
        "# Training loop (one-shot)\n",
        "# -----------------------------\n",
        "for it in range(1, num_iters+1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    # --- Generator forward (create fake)\n",
        "    # re-encode fresh (keeps consistency)\n",
        "    # we can add small noise to content_feat for augmentation, but keep simple\n",
        "    t = adain(content_feat, style_feat)  # target latent for generator\n",
        "\n",
        "    fake = generator(t)  # outputs in [-1,1] due to Tanh\n",
        "\n",
        "    # --------------------------\n",
        "    # Train Discriminator on real vs fake (PatchGAN)\n",
        "    # --------------------------\n",
        "    d_optimizer.zero_grad()\n",
        "\n",
        "    # Real: use style image as \"real\" reference for style? or use content as real?\n",
        "    # We want discriminator to push outputs to be realistic; choose style image as real target for texture\n",
        "    real_inp = augment(style)  # [1,3,H,W]\n",
        "    fake_inp = augment(fake.detach())\n",
        "\n",
        "    d_real = discriminator(real_inp)\n",
        "    d_fake = discriminator(fake_inp)\n",
        "\n",
        "    # targets are patch maps: create tensors with same shape as outputs\n",
        "    real_targets = torch.full_like(d_real, real_label, device=device)\n",
        "    fake_targets = torch.full_like(d_fake, fake_label, device=device)\n",
        "\n",
        "    loss_d_real = mse(d_real, real_targets)\n",
        "    loss_d_fake = mse(d_fake, fake_targets)\n",
        "    loss_d = 0.5 * (loss_d_real + loss_d_fake)\n",
        "    loss_d.backward()\n",
        "    d_optimizer.step()\n",
        "\n",
        "    # --------------------------\n",
        "    # Train Generator (adversarial + perceptual + style + feature-matching)\n",
        "    # --------------------------\n",
        "    g_optimizer.zero_grad()\n",
        "\n",
        "    # Adversarial loss (want discriminator to predict real_label on fake)\n",
        "    d_fake_for_g = discriminator(fake)\n",
        "    adv_targets = torch.full_like(d_fake_for_g, real_label, device=device)\n",
        "    loss_adv = mse(d_fake_for_g, adv_targets)\n",
        "\n",
        "    # Perceptual (content) loss: compare relu4_4 features of fake and content_feat\n",
        "    out_feats = encoder( (fake + 1.0) / 2.0 )  # encoder expects [0,1] input; fake in [-1,1]\n",
        "    loss_content = l1(out_feats['relu4_4'], content_feat.detach())\n",
        "\n",
        "    # Style loss: Gram matrix mismatch between fake features and style features across multiple layers\n",
        "    style_feats = encoder( (style + 1.0) / 2.0 )\n",
        "    loss_style = 0.0\n",
        "    for layer in ['relu1_2','relu2_2','relu3_4','relu4_4']:\n",
        "        gm_fake = gram_matrix(out_feats[layer])\n",
        "        gm_style = gram_matrix(style_feats[layer])\n",
        "        loss_style = loss_style + mse(gm_fake, gm_style)\n",
        "\n",
        "    # Feature matching loss (use intermediate activations from discriminator)\n",
        "    # A simple approximation: compare discriminator activations on real and fake (feature matching)\n",
        "    with torch.no_grad():\n",
        "        d_real_feats = discriminator(real_inp)\n",
        "    d_fake_feats = discriminator(fake)\n",
        "    loss_fm = l1(d_fake_feats, d_real_feats.detach())\n",
        "\n",
        "    # Total generator loss\n",
        "    loss_g = adv_weight * loss_adv + content_weight * loss_content + style_weight * loss_style + fm_weight * loss_fm\n",
        "    loss_g.backward()\n",
        "    g_optimizer.step()\n",
        "\n",
        "    # --------------------------\n",
        "    # Logging / save intermediate outputs\n",
        "    # --------------------------\n",
        "    if it % print_every == 0 or it == 1:\n",
        "        print(f\"Iter {it}/{num_iters} | D_loss: {loss_d.item():.6f} | G_adv: {loss_adv.item():.6f} | Content: {loss_content.item():.6f} | Style: {loss_style.item():.6f} | FM: {loss_fm.item():.6f}\")\n",
        "        # save output image (de-normalize)\n",
        "        out_vis = (fake.detach().clamp(-1,1) + 1.0) / 2.0  # [0,1]\n",
        "        save_image(out_vis, os.path.join(out_dir, f'fake_{it:06d}.png'))\n",
        "        # optionally save real style and content for reference\n",
        "        save_image( (content + 1.0)/2.0 if content.max()<=1.0 else content, os.path.join(out_dir, 'content.png') )\n",
        "        save_image( (style + 1.0)/2.0 if style.max()<=1.0 else style, os.path.join(out_dir, 'style.png') )\n",
        "\n",
        "# -----------------------------\n",
        "# Final save\n",
        "# -----------------------------\n",
        "final = (fake.detach().clamp(-1,1) + 1.0) / 2.0\n",
        "save_image(final, os.path.join(out_dir, 'final_stylized.png'))\n",
        "print(\"Done. Outputs saved to:\", out_dir)\n"
      ]
    }
  ]
}